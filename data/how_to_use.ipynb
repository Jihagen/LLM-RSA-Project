{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliahagen/anaconda3/envs/llm_rsa_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to sys.path\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "\n",
    "from data_preparation import DatasetPreprocessor\n",
    "from data_loaders import * \n",
    "from utils.file_manager import FileManager\n",
    "\n",
    "file_manager = FileManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WiC Dataset: 5428 samples\n",
      "['Do you want to come over to my place later? [SEP] A political system with no place for the less prominent groups.', 'Approach a task. [SEP] To approach the city.', 'Run rogue. [SEP] She ran 10 miles that day.'] [0, 0, 0]\n",
      "WikiText Dataset: 1165029 samples\n",
      "['= Valkyria Chronicles III =', 'Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" .', \"The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n .\"] [0, 0, 0]\n",
      "TREC Dataset: 5452 samples\n",
      "['How did serfdom develop in and then leave Russia ?', 'What films featured the character Popeye Doyle ?', \"How can I find a list of celebrities ' real names ?\"] [2, 1, 2]\n"
     ]
    },
    {
     "ename": "ManualDownloadError",
     "evalue": "The dataset story_cloze with config 2018 requires manual data.\nPlease follow the manual download instructions:\n To use Story Cloze you have to download it manually. Please fill this google form (http://goo.gl/forms/aQz39sdDrO). Complete the form. Then you will receive a download link for the dataset. Load it using: `datasets.load_dataset('story_cloze', data_dir='path/to/folder/folder_name')`\nManual data can be loaded with:\n datasets.load_dataset(\"story_cloze\", data_dir=\"<path/to/manual/data>\")",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mManualDownloadError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39m# Test with the Story Dataset\u001b[39;00m\n\u001b[1;32m     27\u001b[0m preprocessor \u001b[39m=\u001b[39m DatasetPreprocessor(tokenizer, \u001b[39m\"\u001b[39m\u001b[39mthe story dataset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m texts, labels \u001b[39m=\u001b[39m preprocessor\u001b[39m.\u001b[39;49mload_and_prepare(split\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     29\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoaded \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(texts)\u001b[39m}\u001b[39;00m\u001b[39m samples from The story dataset.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[39mprint\u001b[39m(texts[:\u001b[39m3\u001b[39m], labels[:\u001b[39m3\u001b[39m])\n",
      "File \u001b[0;32m~/LLM RSA Project/data/data_preparation.py:56\u001b[0m, in \u001b[0;36mDatasetPreprocessor.load_and_prepare\u001b[0;34m(self, split)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_and_prepare\u001b[39m(\u001b[39mself\u001b[39m, split\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     47\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39m    Load and preprocess the dataset.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39m        Tuple[List[str], List[int]]: Processed texts and labels.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     raw_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset_loader(split)\n\u001b[1;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess_fn(raw_data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer)\n",
      "File \u001b[0;32m~/LLM RSA Project/data/data_loaders.py:56\u001b[0m, in \u001b[0;36mload_story_dataset\u001b[0;34m(split, config)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_story_dataset\u001b[39m(split\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, config\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m2018\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     48\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39m    Load the Story Cloze Dataset with the specified configuration.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39m        List[Dict]: List of dictionaries with context, endings, and labels.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mstory_cloze\u001b[39;49m\u001b[39m\"\u001b[39;49m, config, split\u001b[39m=\u001b[39;49msplit)\n\u001b[1;32m     58\u001b[0m     \u001b[39m# Transform data into context, ending, and label format\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     processed_data \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_rsa_env/lib/python3.9/site-packages/datasets/load.py:2154\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2151\u001b[0m     \u001b[39mreturn\u001b[39;00m builder_instance\u001b[39m.\u001b[39mas_streaming_dataset(split\u001b[39m=\u001b[39msplit)\n\u001b[1;32m   2153\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2154\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[1;32m   2155\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   2156\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   2157\u001b[0m     verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[1;32m   2158\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m   2159\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   2160\u001b[0m )\n\u001b[1;32m   2162\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2163\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[1;32m   2164\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[1;32m   2165\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_rsa_env/lib/python3.9/site-packages/datasets/builder.py:912\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    909\u001b[0m     _dest \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fs\u001b[39m.\u001b[39m_strip_protocol(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_dir) \u001b[39mif\u001b[39;00m is_local \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_dir\n\u001b[1;32m    910\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDownloading and preparing dataset \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_name\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m{\u001b[39;00m_dest\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 912\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_manual_download(dl_manager)\n\u001b[1;32m    914\u001b[0m \u001b[39m# Create a tmp dir and rename to self._output_dir on successful exit.\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[39mwith\u001b[39;00m incomplete_dir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_dir) \u001b[39mas\u001b[39;00m tmp_output_dir:\n\u001b[1;32m    916\u001b[0m     \u001b[39m# Temporarily assign _output_dir to tmp_data_dir to avoid having to forward\u001b[39;00m\n\u001b[1;32m    917\u001b[0m     \u001b[39m# it to every sub function.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_rsa_env/lib/python3.9/site-packages/datasets/builder.py:948\u001b[0m, in \u001b[0;36mDatasetBuilder._check_manual_download\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_manual_download\u001b[39m(\u001b[39mself\u001b[39m, dl_manager):\n\u001b[1;32m    947\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_download_instructions \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m dl_manager\u001b[39m.\u001b[39mmanual_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 948\u001b[0m         \u001b[39mraise\u001b[39;00m ManualDownloadError(\n\u001b[1;32m    949\u001b[0m             textwrap\u001b[39m.\u001b[39mdedent(\n\u001b[1;32m    950\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m    951\u001b[0m \u001b[39m                The dataset \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_name\u001b[39m}\u001b[39;00m\u001b[39m with config \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m requires manual data.\u001b[39m\n\u001b[1;32m    952\u001b[0m \u001b[39m                Please follow the manual download instructions:\u001b[39m\n\u001b[1;32m    953\u001b[0m \u001b[39m                 \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_download_instructions\u001b[39m}\u001b[39;00m\n\u001b[1;32m    954\u001b[0m \u001b[39m                Manual data can be loaded with:\u001b[39m\n\u001b[1;32m    955\u001b[0m \u001b[39m                 datasets.load_dataset(\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepo_id\u001b[39m \u001b[39m\u001b[39mor\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m, data_dir=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<path/to/manual/data>\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m    956\u001b[0m             )\n\u001b[1;32m    957\u001b[0m         )\n",
      "\u001b[0;31mManualDownloadError\u001b[0m: The dataset story_cloze with config 2018 requires manual data.\nPlease follow the manual download instructions:\n To use Story Cloze you have to download it manually. Please fill this google form (http://goo.gl/forms/aQz39sdDrO). Complete the form. Then you will receive a download link for the dataset. Load it using: `datasets.load_dataset('story_cloze', data_dir='path/to/folder/folder_name')`\nManual data can be loaded with:\n datasets.load_dataset(\"story_cloze\", data_dir=\"<path/to/manual/data>\")"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Test with WiC\n",
    "preprocessor = DatasetPreprocessor(tokenizer, \"wic\")\n",
    "texts, labels = preprocessor.load_and_prepare(split=\"train\")\n",
    "print(f\"WiC Dataset: {len(texts)} samples\")\n",
    "print(texts[:3], labels[:5])\n",
    "\n",
    "# Test with TREC\n",
    "preprocessor = DatasetPreprocessor(tokenizer, \"trec\")\n",
    "texts, labels = preprocessor.load_and_prepare(split=\"train\")\n",
    "print(f\"TREC Dataset: {len(texts)} samples\")\n",
    "print(texts[:3], labels[:5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with the Story Dataset\n",
    "preprocessor = DatasetPreprocessor(tokenizer, \"the story dataset\")\n",
    "texts, labels = preprocessor.load_and_prepare(split=\"train\")\n",
    "print(f\"Loaded {len(texts)} samples from The story dataset.\")\n",
    "print(texts[:3], labels[:3])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_rsa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
